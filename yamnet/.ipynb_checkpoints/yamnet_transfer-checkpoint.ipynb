{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with Yamnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/fbnspl/rfcx-rainforest.git\n",
    "%cd rfcx-rainforest/yamnet/\n",
    "! pip install tensorflow-io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M0woGtbhsxQg"
   },
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers.experimental.preprocessing as kp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions.augment import time_mask, freq_mask, mixup_one_hot\n",
    "from functions.metrics import LWLRAP\n",
    "\n",
    "import params as yamnet_params\n",
    "import yamnet as yamnet_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: LRAP Metric and tensorflow audio read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "def get_lrap(X_val, y_one_hot_val, clf):\n",
    "    y_prob = clf.predict_proba(X_val)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    y_val = np.argmax(y_one_hot_val, axis=1)\n",
    "\n",
    "    print('LRAP: %s' % label_ranking_average_precision_score(y_one_hot_val, y_prob))\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "def get_lrap_keras(X_val, y_one_hot_val, clf):\n",
    "    y_prob = clf.predict(X_val)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    y_val = np.argmax(y_one_hot_val, axis=1)\n",
    "\n",
    "    print('LRAP: %s' % label_ranking_average_precision_score(y_one_hot_val, y_prob))\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "def tf_read_audio(path, sr, target_sr):\n",
    "    # Read in the audio.\n",
    "    audio = tfio.audio.AudioIOTensor(path)\n",
    "    audio = tf.squeeze(audio[:], axis=[-1])\n",
    "    audio = tfio.audio.resample(audio, sr, target_sr)\n",
    "    audio = tf.cast(audio, tf.float32)\n",
    "    audio = audio / 32768.0\n",
    "    return audio.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load data dict from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['csvs_train', 'csvs_val', 'X_train', 'y_train', 'y_one_hot_train', 'X_val', 'y_val', 'y_one_hot_val', 'X_60_train', 'y_60_train', 'y_60_one_hot_train', 'X_60_val', 'y_60_val', 'y_60_one_hot_val', 'X_60_test'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data paths and split to sets, sample rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/fabianseipel/aic/data/rfcx-species-audio-detection/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup sample rates\n",
    "sr = 48000\n",
    "target_sr = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data dict\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all csvs\n",
    "csvs_all = sorted(glob.glob(data_path + 'train_tp/*.csv'))\n",
    "\n",
    "# split files to validation and \n",
    "data['csvs_train'], data['csvs_val'] = train_test_split(csvs_all, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get yamnet model with in-built preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph is designed for a sampling rate of 16 kHz, but higher rates should work too.\n",
    "# We also generate scores at a 10 Hz frame rate.\n",
    "# Set up the YAMNet model.\n",
    "params = yamnet_params.Params(sample_rate=16000, patch_hop_seconds=0.1)\n",
    "class_names = yamnet_model.class_names('yamnet_class_map.csv')\n",
    "yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet.load_weights('yamnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get features and labels only from active 1-sec parts of true positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itearet over splits\n",
    "for split in ['train', 'val']:\n",
    "    csvs = data['csvs_' + split]\n",
    "    \n",
    "    # init lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate through csvs\n",
    "    for csv in tqdm(csvs, position=0, leave=True):\n",
    "\n",
    "        # get single csvs\n",
    "        gt = pd.read_csv(csv, index_col=0).to_numpy()\n",
    "\n",
    "        # get frames and classes where true positives are\n",
    "        specs, frames = gt.nonzero()\n",
    "\n",
    "        # load feature embeddings\n",
    "        audio = tf_read_audio(data_path + f'train/{Path(csv).stem}.flac', 48000, 16000)\n",
    "        audio_frames = librosa.util.frame(audio, frame_length=16000, hop_length=16000)\n",
    "\n",
    "        # iterate through active tp frames\n",
    "        for spec, frame in zip(specs, frames):\n",
    "            audio_frame = audio_frames[int(0.02*16000):-int(0.02*16000), frame]\n",
    "            _, _, spec = yamnet(audio_frame)\n",
    "            spec = spec.numpy()\n",
    "            spec = np.expand_dims(spec, axis=-1)\n",
    "            X.append(spec)\n",
    "            y.append(gt[:, frame])\n",
    "\n",
    "\n",
    "    # convert to numpy array and one-hot\n",
    "    data['X_' + split] = np.array(X)\n",
    "    data['y_' + split] = np.argmax(np.array(y), axis=1)\n",
    "    data['y_one_hot_' + split] = np.array(y)\n",
    "    \n",
    "    print(data['X_' + split].shape, data['y_' + split].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get features and labels on whole 60-seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itearet over splits\n",
    "for split in ['train', 'val']:\n",
    "    csvs = data['csvs_' + split]\n",
    "    \n",
    "    # init lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate through csvs\n",
    "    for csv in tqdm(csvs, position=0, leave=True):\n",
    "\n",
    "        # get single csvs\n",
    "        gt = pd.read_csv(csv, index_col=0).to_numpy()\n",
    "        print(gt.shape)\n",
    "        y.append(gt)\n",
    "        \n",
    "        # read audio\n",
    "        audio = tf_read_audio(data_path + f'train/{Path(csv).stem}.flac', 48000, 16000)\n",
    "        audio_frames = librosa.util.frame(audio, frame_length=16000, hop_length=16000)\n",
    "        X_list = []\n",
    "\n",
    "        # iterate through all frames\n",
    "        for frame in range(audio_frames.shape[1]):\n",
    "            audio_frame = audio_frames[int(0.02*16000):-int(0.02*16000), frame]\n",
    "            _, _, spec = yamnet(audio_frame)\n",
    "            spec = spec.numpy()\n",
    "            spec = np.expand_dims(spec, axis=-1)\n",
    "            X_list.append(spec)\n",
    "\n",
    "        X_list = np.array(X_list)\n",
    "        X.append(X_list)\n",
    "\n",
    "\n",
    "    # convert to numpy array and one-hot\n",
    "    data['X_60_' + split] = np.array(X)\n",
    "    data['y_60_' + split] = np.argmax(np.array(y), axis=1)\n",
    "    data['y_60_one_hot_' + split] = np.array(y)\n",
    "    \n",
    "    print(data['X_60_' + split].shape, data['y_60_one_hot_' + split].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init lists\n",
    "X = []\n",
    "\n",
    "# get all true positive csvs\n",
    "files = sorted(glob.glob(data_path + 'test/*.flac'))\n",
    "\n",
    "# iterate through csvs\n",
    "for file in tqdm(files, position=0, leave=True):\n",
    "    \n",
    "    # load feature embeddings\n",
    "    audio = tf_read_audio(file, 48000, 16000)\n",
    "    audio_frames = librosa.util.frame(audio, frame_length=16000, hop_length=16000)\n",
    "    specs = []\n",
    "    \n",
    "    # iterate through all frames\n",
    "    for frame in range(audio_frames.shape[1]):\n",
    "        audio_frame = audio_frames[int(0.02*16000):-int(0.02*16000), frame]\n",
    "        _, _, spec = yamnet(audio_frame)\n",
    "        spec = spec.numpy()\n",
    "        spec = np.expand_dims(spec, axis=-1)\n",
    "        specs.append(spec)\n",
    "        \n",
    "    specs = np.array(specs)\n",
    "    X.append(specs)\n",
    "\n",
    "# convert to numpy array and one-hot\n",
    "data['X_60_test'] = np.array(X)\n",
    "\n",
    "print(data['X_60_test'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save all data dict to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('data/data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save train/val data dict to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = {}\n",
    "data_train['X_train'] = data['X_train']\n",
    "data_train['y_train'] = data['y_train']\n",
    "data_train['y_one_hot_train'] = data['y_one_hot_train']\n",
    "\n",
    "with open('data/data_train.pickle', 'wb') as f:\n",
    "    pickle.dump(data_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "data_val = {}\n",
    "data_val['X_val'] = data['X_val']\n",
    "data_val['y_val'] = data['y_val']\n",
    "data_val['y_one_hot_val'] = data['y_one_hot_val']\n",
    "\n",
    "with open('data/data_val.pickle', 'wb') as f:\n",
    "    pickle.dump(data_val, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get yamnet architecture with pretrained weights and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # The graph is designed for a sampling rate of 16 kHz, but higher rates should work too.\n",
    "    # We also generate scores at a 10 Hz frame rate.\n",
    "    # Set up the YAMNet model.\n",
    "    params = yamnet_params.Params(sample_rate=16000, patch_hop_seconds=0.1)\n",
    "    class_names = yamnet_model.class_names('yamnet_class_map.csv')\n",
    "    yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "    yamnet.load_weights('yamnet.h5')\n",
    "\n",
    "    # get layers from yamnet\n",
    "    layers = [l for l in yamnet.layers]\n",
    "    core_layers = layers[79:-2]\n",
    "\n",
    "    # add new imput layer\n",
    "    input_layer = tf.keras.Input(shape=(96, 64, 1), name='Input')\n",
    "    x = kp.RandomContrast(factor=0.2)(input_layer)\n",
    "\n",
    "    # attach layer again from convolutions on\n",
    "    for i, layer in enumerate(core_layers):\n",
    "        x = layer(x)\n",
    "        \n",
    "    # add new prediction layer\n",
    "    x = tf.keras.layers.Dense(24, activation='sigmoid')(x)\n",
    "\n",
    "    # construct model\n",
    "    yamnet_tl = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    '''\n",
    "    # freeze some layers \n",
    "    for layer in yamnet_tl.layers[:50]:\n",
    "        layer.trainable =  False\n",
    "    '''\n",
    "    \n",
    "    return yamnet_tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 96, 64, 1), (None, 24)), types: (tf.float32, tf.float32)>\n",
      "(3287, 96, 64, 1)\n",
      "(3287, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor el in train_dataset:\\n    print(el)\\n    break\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autotune computation\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data['X_train'], data['y_one_hot_train']))\n",
    "n_mels, n_frames, n_channels = train_dataset.element_spec[0].shape\n",
    "# print(train_dataset)\n",
    "\n",
    "\n",
    "# train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2048)\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (tf.squeeze(mel_spec, axis=2), y), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (tf.cast(mel_spec, tf.float32), tf.cast(y, tf.float32)), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (tf.roll(mel_spec, tf.random.uniform((), minval=-15, maxval=15, dtype=tf.dtypes.int32), axis=1), y), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(32)\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: mixup_one_hot(mel_spec, y, 0.5), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.unbatch()\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (time_mask(mel_spec, param=int(n_frames * 0.1)), y), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (time_mask(mel_spec, param=int(n_frames * 0.1)), y), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (freq_mask(mel_spec, param=int(n_mels * 0.1)), y), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (freq_mask(mel_spec, param=int(n_mels * 0.1)), y), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (freq_mask(mel_spec, param=int(n_mels * 0.1)), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda mel_spec, y: (tf.expand_dims(mel_spec, axis=2), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "print(train_dataset)\n",
    "print(data['X_train'].shape)\n",
    "print(data['y_one_hot_train'].shape)\n",
    "\n",
    "'''\n",
    "for el in train_dataset:\n",
    "    print(el)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "103/103 [==============================] - 24s 230ms/step - loss: 2.9404 - lwlrap: 0.2011 - precision_1: 0.1634 - recall_1: 0.4021 - categorical_accuracy: 0.1841 - val_loss: 7.7119 - val_lwlrap: 0.3060 - val_precision_1: 0.0995 - val_recall_1: 0.3713 - val_categorical_accuracy: 0.1425\n",
      "Epoch 2/50\n",
      "103/103 [==============================] - 24s 238ms/step - loss: 2.8338 - lwlrap: 0.2130 - precision_1: 0.1967 - recall_1: 0.3713 - categorical_accuracy: 0.1886 - val_loss: 3.7054 - val_lwlrap: 0.3668 - val_precision_1: 0.1019 - val_recall_1: 0.5760 - val_categorical_accuracy: 0.2120\n",
      "Epoch 3/50\n",
      "103/103 [==============================] - 23s 228ms/step - loss: 2.7898 - lwlrap: 0.2192 - precision_1: 0.2087 - recall_1: 0.3617 - categorical_accuracy: 0.1947 - val_loss: 3.1768 - val_lwlrap: 0.3635 - val_precision_1: 0.1216 - val_recall_1: 0.4754 - val_categorical_accuracy: 0.1880\n",
      "Epoch 4/50\n",
      "103/103 [==============================] - 24s 231ms/step - loss: 2.7701 - lwlrap: 0.2209 - precision_1: 0.2091 - recall_1: 0.3709 - categorical_accuracy: 0.1935 - val_loss: 3.8316 - val_lwlrap: 0.3554 - val_precision_1: 0.0902 - val_recall_1: 0.4443 - val_categorical_accuracy: 0.2228\n",
      "Epoch 5/50\n",
      "103/103 [==============================] - 24s 237ms/step - loss: 2.7348 - lwlrap: 0.2246 - precision_1: 0.2157 - recall_1: 0.3841 - categorical_accuracy: 0.1959 - val_loss: 4.3662 - val_lwlrap: 0.3325 - val_precision_1: 0.0948 - val_recall_1: 0.5892 - val_categorical_accuracy: 0.1545\n",
      "Epoch 6/50\n",
      "103/103 [==============================] - 26s 251ms/step - loss: 2.7408 - lwlrap: 0.2240 - precision_1: 0.2125 - recall_1: 0.3983 - categorical_accuracy: 0.1981 - val_loss: 6.2491 - val_lwlrap: 0.2788 - val_precision_1: 0.0885 - val_recall_1: 0.5928 - val_categorical_accuracy: 0.1030\n",
      "Epoch 7/50\n",
      "103/103 [==============================] - 24s 236ms/step - loss: 2.7069 - lwlrap: 0.2306 - precision_1: 0.2268 - recall_1: 0.3901 - categorical_accuracy: 0.2063 - val_loss: 4.3800 - val_lwlrap: 0.3199 - val_precision_1: 0.0919 - val_recall_1: 0.4838 - val_categorical_accuracy: 0.1641\n",
      "Epoch 8/50\n",
      "103/103 [==============================] - 24s 234ms/step - loss: 2.6901 - lwlrap: 0.2338 - precision_1: 0.2245 - recall_1: 0.3891 - categorical_accuracy: 0.2169 - val_loss: 2.9689 - val_lwlrap: 0.4109 - val_precision_1: 0.1284 - val_recall_1: 0.5629 - val_categorical_accuracy: 0.2251\n",
      "Epoch 9/50\n",
      "103/103 [==============================] - 24s 235ms/step - loss: 2.6680 - lwlrap: 0.2397 - precision_1: 0.2338 - recall_1: 0.3873 - categorical_accuracy: 0.2276 - val_loss: 5.0524 - val_lwlrap: 0.3374 - val_precision_1: 0.1040 - val_recall_1: 0.4335 - val_categorical_accuracy: 0.1772\n",
      "Epoch 10/50\n",
      "103/103 [==============================] - 24s 228ms/step - loss: 2.6438 - lwlrap: 0.2471 - precision_1: 0.2483 - recall_1: 0.3752 - categorical_accuracy: 0.2467 - val_loss: 3.3241 - val_lwlrap: 0.4083 - val_precision_1: 0.1569 - val_recall_1: 0.4611 - val_categorical_accuracy: 0.2275\n",
      "Epoch 11/50\n",
      "103/103 [==============================] - 24s 237ms/step - loss: 2.6231 - lwlrap: 0.2541 - precision_1: 0.2578 - recall_1: 0.3352 - categorical_accuracy: 0.2638 - val_loss: 2.9908 - val_lwlrap: 0.3979 - val_precision_1: 0.1395 - val_recall_1: 0.4599 - val_categorical_accuracy: 0.2275\n",
      "Epoch 12/50\n",
      "103/103 [==============================] - 24s 236ms/step - loss: 2.6061 - lwlrap: 0.2525 - precision_1: 0.2702 - recall_1: 0.3242 - categorical_accuracy: 0.2644 - val_loss: 2.7784 - val_lwlrap: 0.4121 - val_precision_1: 0.1507 - val_recall_1: 0.4455 - val_categorical_accuracy: 0.2311\n",
      "Epoch 13/50\n",
      "103/103 [==============================] - 24s 231ms/step - loss: 2.5768 - lwlrap: 0.2541 - precision_1: 0.2747 - recall_1: 0.3112 - categorical_accuracy: 0.2619 - val_loss: 3.7971 - val_lwlrap: 0.3754 - val_precision_1: 0.1569 - val_recall_1: 0.3030 - val_categorical_accuracy: 0.2204\n",
      "Epoch 14/50\n",
      "103/103 [==============================] - 23s 225ms/step - loss: 2.5761 - lwlrap: 0.2572 - precision_1: 0.2743 - recall_1: 0.3121 - categorical_accuracy: 0.2644 - val_loss: 3.0352 - val_lwlrap: 0.3884 - val_precision_1: 0.1622 - val_recall_1: 0.3257 - val_categorical_accuracy: 0.2036\n",
      "Epoch 15/50\n",
      "103/103 [==============================] - 25s 241ms/step - loss: 2.5508 - lwlrap: 0.2640 - precision_1: 0.2909 - recall_1: 0.2965 - categorical_accuracy: 0.2845 - val_loss: 2.7429 - val_lwlrap: 0.4234 - val_precision_1: 0.1851 - val_recall_1: 0.3581 - val_categorical_accuracy: 0.2443\n",
      "Epoch 16/50\n",
      "103/103 [==============================] - 25s 246ms/step - loss: 2.5601 - lwlrap: 0.2659 - precision_1: 0.3043 - recall_1: 0.2791 - categorical_accuracy: 0.3003 - val_loss: 3.2393 - val_lwlrap: 0.3770 - val_precision_1: 0.1954 - val_recall_1: 0.2419 - val_categorical_accuracy: 0.2084\n",
      "Epoch 17/50\n",
      "103/103 [==============================] - 25s 245ms/step - loss: 2.5076 - lwlrap: 0.2743 - precision_1: 0.3229 - recall_1: 0.2651 - categorical_accuracy: 0.3030 - val_loss: 3.2480 - val_lwlrap: 0.4144 - val_precision_1: 0.1877 - val_recall_1: 0.2012 - val_categorical_accuracy: 0.2587\n",
      "Epoch 18/50\n",
      " 56/103 [===============>..............] - ETA: 10s - loss: 2.4508 - lwlrap: 0.2804 - precision_1: 0.3358 - recall_1: 0.2799 - categorical_accuracy: 0.3192"
     ]
    }
   ],
   "source": [
    "# get model\n",
    "yamnet_tl = build_model()\n",
    "# yamnet_tl.summary()\n",
    "\n",
    "# lwrap metric\n",
    "metrics = [LWLRAP(num_classes=24), \n",
    "           tf.metrics.Precision(), \n",
    "           tf.metrics.Recall(), \n",
    "           tf.metrics.CategoricalAccuracy()]\n",
    "\n",
    "# optimizer\n",
    "opt = tf.keras.optimizers.Nadam(lr=0.001, clipnorm=1.0)\n",
    "\n",
    "# compile model\n",
    "yamnet_tl.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=metrics)\n",
    "# train model\n",
    "yamnet_tl.fit(\n",
    "              # data['X_train'], data['y_one_hot_train'],\n",
    "              train_dataset,\n",
    "              epochs=50, \n",
    "              verbose=1,\n",
    "              validation_data=(data['X_val'], data['y_one_hot_val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict on 60-seconds features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_val = []\n",
    "X_60_val = data['X_60_val']\n",
    "\n",
    "for n in tqdm(range(X_60_val.shape[0]), position=0, leave=True):\n",
    "    y_prob = yamnet_tl.predict(X_60_val[n, :])\n",
    "    y_prob_val.append(y_prob)\n",
    "\n",
    "    \n",
    "y_prob_val = np.array(y_prob_val)\n",
    "y_prob_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate probabilities over 60-sec with max an mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_val_max = np.max(y_prob_val, axis=1)\n",
    "y_prob_val_mean = np.mean(y_prob_val, axis=1)\n",
    "y_prob_val_min = np.min(y_prob_val, axis=1)\n",
    "y_prob_val_max.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get ground truth, aggregate over 60-sec with max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_60_one_hot_val = data['y_60_one_hot_val']\n",
    "y_60_one_hot_val_max = np.max(y_60_one_hot_val, axis=2)\n",
    "y_60_one_hot_val_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LRAP Max  : %s' % label_ranking_average_precision_score(y_60_one_hot_val_max, y_prob_val_max))\n",
    "print('LRAP Mean : %s' % label_ranking_average_precision_score(y_60_one_hot_val_max, y_prob_val_mean))\n",
    "print('LRAP Min. : %s' % label_ranking_average_precision_score(y_60_one_hot_val_max, y_prob_val_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prob = yamnet_tl.predict(X_train[3, :])\n",
    "y_test_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(y_test_prob)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "yamnet_visualization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rainforest",
   "language": "python",
   "name": "rainforest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
